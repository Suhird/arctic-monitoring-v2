{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Comprehensive evaluation of trained ice classification models:\n",
    "- Load trained model\n",
    "- Evaluate on test set\n",
    "- Generate confusion matrix\n",
    "- Calculate per-class metrics\n",
    "- Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append('../training')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import dataset and model\n",
    "from train_ice_classifier import IceDataset, IceClassifier\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration\n",
    "MODEL_PATH = '../models/ice_classifier_resnet50.pth'\n",
    "DATA_DIR = '../data/processed'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load model\n",
    "model = IceClassifier(num_classes=3, pretrained=False)\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ… Model loaded from: {MODEL_PATH}\")\n",
    "print(f\"   Epoch: {checkpoint['epoch']}\")\n",
    "print(f\"   Val Loss: {checkpoint['val_loss']:.4f}\")\n",
    "print(f\"   Val Acc: {checkpoint['val_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load test dataset\n",
    "test_dataset = IceDataset(DATA_DIR, split='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "\n",
    "print(\"Evaluating model on test set...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc='Testing'):\n",
    "        images = images.to(DEVICE)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probabilities = np.array(all_probabilities)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_labels, all_predictions) * 100\n",
    "print(f\"\\nâœ… Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "class_names = ['Open Water', 'Thin Ice', 'Thick Ice']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Per-Class Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Classification report\n",
    "report = classification_report(all_labels, all_predictions, \n",
    "                               target_names=class_names, digits=3)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(report)\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_mask = all_labels == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = (all_predictions[class_mask] == all_labels[class_mask]).mean() * 100\n",
    "        print(f\"  {class_name}: {class_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find misclassified samples\n",
    "errors = all_predictions != all_labels\n",
    "error_indices = np.where(errors)[0]\n",
    "\n",
    "print(f\"Total errors: {len(error_indices)} / {len(all_labels)} ({len(error_indices)/len(all_labels)*100:.1f}%)\")\n",
    "\n",
    "# Visualize some errors\n",
    "if len(error_indices) > 0:\n",
    "    num_errors_to_show = min(6, len(error_indices))\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, idx in enumerate(error_indices[:num_errors_to_show]):\n",
    "        # Load image\n",
    "        image, _ = test_dataset[idx]\n",
    "        img_display = image.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Plot\n",
    "        axes[i].imshow(img_display)\n",
    "        axes[i].set_title(f\"True: {class_names[all_labels[idx]]}\\nPred: {class_names[all_predictions[idx]]}\\n({all_probabilities[idx, all_predictions[idx]]*100:.1f}% conf)\",\n",
    "                         fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Misclassified Samples', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ðŸŽ‰ Perfect accuracy! No errors to show.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confidence Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get confidence scores\n",
    "confidences = np.max(all_probabilities, axis=1)\n",
    "correct_confidences = confidences[~errors]\n",
    "incorrect_confidences = confidences[errors]\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Overall confidence distribution\n",
    "ax1.hist(confidences, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax1.set_xlabel('Confidence', fontsize=11)\n",
    "ax1.set_ylabel('Count', fontsize=11)\n",
    "ax1.set_title('Prediction Confidence Distribution', fontsize=12, fontweight='bold')\n",
    "ax1.axvline(np.mean(confidences), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(confidences):.3f}')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Correct vs incorrect\n",
    "if len(incorrect_confidences) > 0:\n",
    "    ax2.hist([correct_confidences, incorrect_confidences], bins=15, \n",
    "            label=['Correct', 'Incorrect'], color=['green', 'red'],\n",
    "            alpha=0.6, edgecolor='black')\n",
    "    ax2.set_xlabel('Confidence', fontsize=11)\n",
    "    ax2.set_ylabel('Count', fontsize=11)\n",
    "    ax2.set_title('Confidence: Correct vs Incorrect', fontsize=12, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No errors!', ha='center', va='center', fontsize=16)\n",
    "    ax2.set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage confidence (correct): {np.mean(correct_confidences):.3f}\")\n",
    "if len(incorrect_confidences) > 0:\n",
    "    print(f\"Average confidence (incorrect): {np.mean(incorrect_confidences):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Model evaluation complete!\n",
    "\n",
    "**Key Metrics**:\n",
    "- Test Accuracy: See output above\n",
    "- Confusion Matrix: Visualized above\n",
    "- Per-class performance: Check classification report\n",
    "\n",
    "**Next Steps**:\n",
    "- If accuracy is low, try:\n",
    "  - More training data\n",
    "  - Longer training\n",
    "  - Data augmentation\n",
    "- If ready, deploy model to production\n",
    "- Continue to `04_predictions_visualization.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
